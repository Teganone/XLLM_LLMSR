nohup: ignoring input
2025-04-26 18:16:36,691 - run_pipeline - INFO - Namespace(input='results/Test_A_results_o3-mini-high_oneshot.json', output='results/Test_A_results.json', config=None, batch_size=10, max_retries=3, combined=True, combined_parser='icl', combined_model='/datacenter/chendanchun/models/finetune/Llama-3-8B-Instruct_o3-mini-high_combined/final_model', combined_params='temperature=0.6', qp=False, qp_parser='icl', qp_model='gpt-4', qp_params=None, cp=False, cp_parser='icl', cp_model='gpt-4', cp_params=None, verify=True, verifier='llm', verifier_model='o3-mini', verifier_params='reasoning_effort=high', temperature=0.7, top_p=0.9, max_tokens=1024)
2025-04-26 18:16:36,691 - pipeline_runner - INFO - åŠ è½½æ•°æ®: results/Test_A_results_o3-mini-high_oneshot.json
2025-04-26 18:16:36,692 - pipeline_runner - INFO - è¿è¡Œ Pipeline: LLMSR_Pipeline
2025-04-26 18:16:36,692 - pipeline_builder - INFO - è¿è¡ŒèŠ‚ç‚¹: combined_parser
2025-04-26 18:16:36,692 - parser_node - INFO - åŠ è½½Llamaæ¨¡å‹: /datacenter/chendanchun/models/finetune/Llama-3-8B-Instruct_o3-mini-high_combined/final_model
2025-04-26 18:16:36,692 - llama_model - INFO - æ­£åœ¨åŠ è½½æ¨¡å‹: /datacenter/chendanchun/models/finetune/Llama-3-8B-Instruct_o3-mini-high_combined/final_model
2025-04-26 18:16:37,436 - llama_model - INFO - ä½¿ç”¨è®¾å¤‡: cuda

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.43s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:03<00:03,  1.57s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.60s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.23s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.35s/it]
Device set to use cuda:0
2025-04-26 18:16:45,299 - llama_model - INFO - æ¨¡å‹åŠ è½½å®Œæˆï¼
2025-04-26 18:16:45,300 - parser_node - INFO - åˆ›å»ºICLè§£æå™¨ï¼Œä»»åŠ¡: combined
prompts/extract_combined.txt
2025-04-26 18:16:45,300 - parser_node - INFO - è¿è¡Œcombined_parserè§£æå™¨

å¤„ç†æµ‹è¯•æ•°æ®:   0%|          | 0/50 [00:00<?, ?it/s]
å¤„ç†æµ‹è¯•æ•°æ®:   2%|â–         | 1/50 [00:34<28:02, 34.34s/it]
å¤„ç†æµ‹è¯•æ•°æ®:   4%|â–         | 2/50 [01:01<24:07, 30.15s/it]
å¤„ç†æµ‹è¯•æ•°æ®:   6%|â–Œ         | 3/50 [01:55<31:59, 40.84s/it]
å¤„ç†æµ‹è¯•æ•°æ®:   8%|â–Š         | 4/50 [02:30<29:47, 38.86s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  10%|â–ˆ         | 5/50 [03:23<32:55, 43.91s/it]2025-04-26 18:20:09,488 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): CUDA out of memory. Tried to allocate 62.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 60.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 151.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:20:11,857 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): CUDA out of memory. Tried to allocate 62.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 155.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:20:13,903 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): CUDA out of memory. Tried to allocate 62.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 155.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:20:13,904 - llama_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 155.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

å¤„ç†æµ‹è¯•æ•°æ®:  12%|â–ˆâ–        | 6/50 [03:28<22:26, 30.61s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  14%|â–ˆâ–        | 7/50 [04:00<22:17, 31.11s/it]2025-04-26 18:20:46,426 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 158.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
2025-04-26 18:20:48,469 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 158.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:20:50,515 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 158.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:20:50,515 - llama_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 158.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

å¤„ç†æµ‹è¯•æ•°æ®:  16%|â–ˆâ–Œ        | 8/50 [04:05<15:50, 22.63s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  18%|â–ˆâ–Š        | 9/50 [04:34<16:58, 24.85s/it]2025-04-26 18:21:20,264 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 167.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:21:22,318 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 167.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:21:24,364 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 167.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:21:24,364 - llama_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 167.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

å¤„ç†æµ‹è¯•æ•°æ®:  20%|â–ˆâ–ˆ        | 10/50 [04:39<12:18, 18.45s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  22%|â–ˆâ–ˆâ–       | 11/50 [05:15<15:36, 24.02s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  24%|â–ˆâ–ˆâ–       | 12/50 [05:48<16:55, 26.73s/it]å·²ä¿å­˜10ä¸ªç»“æœåˆ°results/Test_A_results_combined_step1_20250426_181636.json
2025-04-26 18:22:34,298 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 165.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:22:36,344 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 165.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:22:38,390 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 165.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:22:38,390 - llama_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 165.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

å¤„ç†æµ‹è¯•æ•°æ®:  26%|â–ˆâ–ˆâ–Œ       | 13/50 [05:53<12:19, 19.98s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  28%|â–ˆâ–ˆâ–Š       | 14/50 [06:24<14:00, 23.34s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [07:13<18:07, 31.07s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [08:18<23:24, 41.31s/it]2025-04-26 18:25:03,631 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 168.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:25:05,666 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 168.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:25:07,711 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 168.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:25:07,711 - llama_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 168.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

å¤„ç†æµ‹è¯•æ•°æ®:  34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [08:22<16:34, 30.13s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [09:04<18:02, 33.83s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [09:40<17:43, 34.29s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [10:17<17:38, 35.28s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [11:13<19:57, 41.29s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [12:09<21:20, 45.72s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [12:57<20:55, 46.49s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [13:45<20:19, 46.91s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [14:35<19:54, 47.78s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [15:19<18:42, 46.77s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [15:59<17:11, 44.84s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [16:30<14:54, 40.67s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [17:21<15:16, 43.64s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [17:53<13:24, 40.23s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [18:40<13:22, 42.26s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [19:21<12:30, 41.69s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [20:00<11:34, 40.88s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [20:37<10:37, 39.86s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [21:09<09:20, 37.35s/it]å·²ä¿å­˜20ä¸ªç»“æœåˆ°results/Test_A_results_combined_step1_20250426_181636.json
å·²ä¿å­˜30ä¸ªç»“æœåˆ°results/Test_A_results_combined_step1_20250426_181636.json
2025-04-26 18:37:54,354 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 156.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:37:56,381 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 156.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:37:58,425 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 156.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:37:58,425 - llama_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.26 GiB is allocated by PyTorch, and 156.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

å¤„ç†æµ‹è¯•æ•°æ®:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [21:13<06:23, 27.38s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [21:45<06:15, 28.86s/it]2025-04-26 18:38:30,770 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 168.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:38:32,813 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 168.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:38:34,860 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 168.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:38:34,860 - llama_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.24 GiB is allocated by PyTorch, and 168.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

å¤„ç†æµ‹è¯•æ•°æ®:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [21:49<04:17, 21.44s/it]2025-04-26 18:38:35,543 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 159.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:38:37,588 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 159.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:38:39,616 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 159.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:38:39,616 - llama_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 159.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

å¤„ç†æµ‹è¯•æ•°æ®:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [21:54<03:00, 16.44s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [22:18<03:07, 18.78s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [23:03<04:00, 26.67s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [23:29<03:31, 26.49s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [24:07<03:30, 30.02s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [24:43<03:10, 31.69s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [25:29<02:59, 35.96s/it]å·²ä¿å­˜40ä¸ªç»“æœåˆ°results/Test_A_results_combined_step1_20250426_181636.json
2025-04-26 18:42:14,812 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 166.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:42:16,838 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 166.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:42:18,882 - llama_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 166.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 18:42:18,882 - llama_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 56.38 MiB is free. Process 1856546 has 92.18 GiB memory in use. Including non-PyTorch memory, this process has 2.84 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 166.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

å¤„ç†æµ‹è¯•æ•°æ®:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [25:33<01:45, 26.40s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [26:08<01:26, 28.86s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [26:49<01:04, 32.48s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [27:30<00:35, 35.12s/it]
å¤„ç†æµ‹è¯•æ•°æ®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [28:18<00:00, 39.03s/it]
å¤„ç†æµ‹è¯•æ•°æ®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [28:18<00:00, 33.97s/it]
å·²ä¿å­˜50ä¸ªç»“æœåˆ°results/Test_A_results_combined_step1_20250426_181636.json
å¤„ç†å®Œæˆï¼Œå…±50ä¸ªç»“æœå·²ä¿å­˜åˆ°results/Test_A_results_combined_step1_20250426_181636.json
2025-04-26 18:45:03,851 - pipeline_builder - INFO - è¿è¡ŒèŠ‚ç‚¹: verifier
2025-04-26 18:45:03,851 - verifier_node - INFO - åŠ è½½OpenAIæ¨¡å‹: o3-mini
2025-04-26 18:45:03,888 - openai_model - INFO - GPTæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: o3-mini
2025-04-26 18:45:03,888 - verifier_node - INFO - åˆ›å»ºLLMéªŒè¯å™¨
2025-04-26 18:45:03,888 - verifier_node - INFO - è¿è¡ŒverifieréªŒè¯å™¨

å¤„ç†æµ‹è¯•æ•°æ®:   0%|          | 0/50 [00:00<?, ?it/s]
å¤„ç†æµ‹è¯•æ•°æ®:   2%|â–         | 1/50 [00:27<22:51, 27.99s/it]
å¤„ç†æµ‹è¯•æ•°æ®:   4%|â–         | 2/50 [01:33<40:02, 50.05s/it]
å¤„ç†æµ‹è¯•æ•°æ®:   6%|â–Œ         | 3/50 [04:33<1:25:43, 109.43s/it]
å¤„ç†æµ‹è¯•æ•°æ®:   8%|â–Š         | 4/50 [05:19<1:04:39, 84.33s/it] 
å¤„ç†æµ‹è¯•æ•°æ®:  10%|â–ˆ         | 5/50 [06:51<1:05:15, 87.02s/it]
å¤„ç†æµ‹è¯•æ•°æ®:  14%|â–ˆâ–        | 7/50 [07:30<37:47, 52.73s/it]  
å¤„ç†æµ‹è¯•æ•°æ®:  18%|â–ˆâ–Š        | 9/50 [07:54<24:27, 35.79s/it]
å¤„ç†æµ‹è¯•æ•°æå¤„ç†æµ‹è¯•æ•°æ®:  28%|â–ˆâ–ˆâ–Š       | 14/50 [11:28<24:18, 40.51s/it]å¤„ç†æµ‹è¯•æ•°æ®:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [12:22<25:14, 43.28s/it]å¤„ç†æµ‹è¯•æ•°æ®:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [19:02<1:11:35, 126.35s/it]å¤„ç†æµ‹è¯•æ•°æ®:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [22:04<59:47, 112.12s/it]  å¤„ç†æµ‹è¯•æ•°æ®:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [23:32<55:10, 106.78s/it]å¤„ç†æµ‹è¯•æ•°æ®:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [26:10<59:36, 119.21s/it]å¤„ç†æµ‹è¯•æ•°æ®:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [27:00<49:03, 101.50s/it]å¤„ç†æµ‹è¯•æ•°æ®:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [28:28<45:36, 97.75s/it] å·²ä¿å­˜10ä¸ªç»“æœåˆ°results/Test_A_results_final_20250426_181636.json
å·²ä¿å­˜20ä¸ªç»“æœåˆ°results/Test_A_results_final_20250426_181636.json
2025-04-26 19:13:44,727 - openai_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
2025-04-26 19:14:58,353 - openai_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
2025-04-26 19:15:23,320 - openai_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
2025-04-26 19:15:23,320 - openai_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
response not str: {}
2025-04-26 19:15:50,530 - openai_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
2025-04-26 19:16:31,962 - openai_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
2025-04-26 19:16:56,877 - openai_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
2025-04-26 19:16:56,877 - openai_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
response not str: {}
2025-04-26 19:17:40,842 - openai_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 1/3): Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
2025-04-26 19:18:18,459 - openai_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 2/3): Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
2025-04-26 19:18:51,925 - openai_model - INFO - APIè°ƒç”¨å¤±è´¥ (å°è¯• 3/3): Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
2025-04-26 19:18:51,925 - openai_model - INFO - ç”Ÿæˆå“åº”å¤±è´¥: Error code: 400 - {'error': {'message': "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}
å¤„ç†æµ‹è¯•æ•°æ®:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [33:48<1:11:37, 159.15s/it]å¤„ç†æµ‹è¯•æ•°æ®:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [34:36<55:22, 127.79s/it]  å¤„ç†æµ‹è¯•æ•°æ®:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [34:57<40:27, 97.11s/it] 