nohup: ignoring input
2025-04-26 15:42:52,097 - run_pipeline - INFO - Namespace(input='data/Final_Selection_Train_v2.json', output='results/Train_llama_0.6_0.3_0.1_results.json', config=None, batch_size=10, max_retries=3, combined=True, combined_parser='icl', combined_model='/datacenter/models/LLM-Research/Llama-3-8B-Instruct', combined_params='temperature=0.6', qp=True, qp_parser='icl', qp_model='/datacenter/models/LLM-Research/Llama-3-8B-Instruct', qp_params='temperature=0.3', cp=False, cp_parser='icl', cp_model='gpt-4', cp_params=None, verify=True, verifier='llm', verifier_model='/datacenter/models/LLM-Research/Llama-3-8B-Instruct', verifier_params='temperature=0.1', temperature=0.7, top_p=0.9, max_tokens=1024)
2025-04-26 15:42:52,098 - pipeline_runner - INFO - 加载数据: data/Final_Selection_Train_v2.json
2025-04-26 15:42:52,098 - pipeline_runner - INFO - 运行 Pipeline: LLMSR_Pipeline
2025-04-26 15:42:52,098 - pipeline_builder - INFO - 运行节点: combined_parser
2025-04-26 15:42:52,098 - parser_node - INFO - 加载Llama模型: /datacenter/models/LLM-Research/Llama-3-8B-Instruct
2025-04-26 15:42:52,099 - llama_model - INFO - 正在加载模型: /datacenter/models/LLM-Research/Llama-3-8B-Instruct
2025-04-26 15:42:52,811 - llama_model - INFO - 使用设备: cuda
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
Device set to use cuda:0
2025-04-26 15:43:00,905 - llama_model - INFO - 模型加载完成！
2025-04-26 15:43:00,905 - parser_node - INFO - 创建ICL解析器，任务: combined
prompts/extract_combined.txt
2025-04-26 15:43:00,905 - parser_node - INFO - 运行combined_parser解析器
处理测试数据:   0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:01,948 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 16.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 6.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 12.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:03,990 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 6.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.11 GiB is allocated by PyTorch, and 20.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:06,024 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 6.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.11 GiB is allocated by PyTorch, and 20.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:06,024 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 6.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.56 GiB memory in use. Of the allocated memory 2.11 GiB is allocated by PyTorch, and 20.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:   4%|▍         | 1/24 [00:05<01:57,  5.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:06,699 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:08,744 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:10,784 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:10,784 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:   8%|▊         | 2/24 [00:09<01:47,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:11,417 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 38.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 34.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:13,445 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 38.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 34.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:15,493 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 38.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 34.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:15,493 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 34.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  12%|█▎        | 3/24 [00:14<01:41,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:16,074 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:18,108 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:20,150 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:20,150 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  17%|█▋        | 4/24 [00:19<01:35,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:20,464 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:22,504 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:24,546 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:24,546 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  21%|██        | 5/24 [00:23<01:27,  4.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:24,855 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:26,878 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:28,918 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:28,918 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  25%|██▌       | 6/24 [00:28<01:21,  4.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:29,461 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:31,502 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:33,545 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:33,545 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  29%|██▉       | 7/24 [00:32<01:17,  4.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:33,849 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:35,879 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:37,920 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:37,921 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  33%|███▎      | 8/24 [00:37<01:12,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:38,239 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:40,280 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:42,305 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:42,305 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  38%|███▊      | 9/24 [00:41<01:07,  4.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:42,641 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:44,681 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:46,723 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:46,723 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  42%|████▏     | 10/24 [00:45<01:02,  4.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
已保存10个结果到results/Train_llama_0.6_0.3_0.1_results_combined_step1_20250426_154252.json
2025-04-26 15:43:47,034 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:49,074 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:51,114 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:51,114 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  46%|████▌     | 11/24 [00:50<00:57,  4.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:51,675 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:53,716 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:55,755 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:43:55,755 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  50%|█████     | 12/24 [00:54<00:53,  4.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:56,068 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:43:58,108 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:00,139 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:00,139 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  54%|█████▍    | 13/24 [00:59<00:49,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:00,696 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 36.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 35.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:02,737 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 36.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 35.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:04,785 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 36.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 35.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:04,785 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 35.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  58%|█████▊    | 14/24 [01:03<00:45,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:05,139 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:07,170 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:09,202 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:09,202 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  62%|██████▎   | 15/24 [01:08<00:40,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:09,756 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:11,804 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:13,851 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:13,851 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  67%|██████▋   | 16/24 [01:12<00:36,  4.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:14,189 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:16,239 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:18,288 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:18,288 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  71%|███████   | 17/24 [01:17<00:31,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:18,812 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 30.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 38.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:20,859 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 30.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 38.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:22,906 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 30.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 38.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:22,907 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 38.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  75%|███████▌  | 18/24 [01:22<00:27,  4.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:23,224 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:25,273 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:27,304 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:27,304 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 36.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  79%|███████▉  | 19/24 [01:26<00:22,  4.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:27,642 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:29,688 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:31,735 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:31,736 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  83%|████████▎ | 20/24 [01:30<00:17,  4.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
已保存20个结果到results/Train_llama_0.6_0.3_0.1_results_combined_step1_20250426_154252.json
2025-04-26 15:44:32,064 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:34,112 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:36,160 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:36,160 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  88%|████████▊ | 21/24 [01:35<00:13,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:36,485 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:38,521 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:40,570 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:40,570 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  92%|█████████▏| 22/24 [01:39<00:08,  4.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:40,889 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:42,935 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:44,982 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:44,982 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 37.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据:  96%|█████████▌| 23/24 [01:44<00:04,  4.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:45,503 - llama_model - INFO - API调用失败 (尝试 1/3): CUDA out of memory. Tried to allocate 30.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 38.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:47,534 - llama_model - INFO - API调用失败 (尝试 2/3): CUDA out of memory. Tried to allocate 30.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 38.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
2025-04-26 15:44:49,580 - llama_model - INFO - API调用失败 (尝试 3/3): CUDA out of memory. Tried to allocate 30.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 38.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-04-26 15:44:49,580 - llama_model - INFO - 生成响应失败: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 4 has a total capacity of 95.10 GiB of which 22.38 MiB is free. Process 1850802 has 92.51 GiB memory in use. Including non-PyTorch memory, this process has 2.55 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 38.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
处理测试数据: 100%|██████████| 24/24 [01:48<00:00,  4.48s/it]处理测试数据: 100%|██████████| 24/24 [01:48<00:00,  4.53s/it]
处理完成，共24个结果已保存到results/Train_llama_0.6_0.3_0.1_results_combined_step1_20250426_154252.json
2025-04-26 15:44:49,582 - pipeline_builder - INFO - 运行节点: qp_parser
2025-04-26 15:44:49,582 - parser_node - INFO - 加载Llama模型: /datacenter/models/LLM-Research/Llama-3-8B-Instruct
2025-04-26 15:44:49,582 - llama_model - INFO - 正在加载模型: /datacenter/models/LLM-Research/Llama-3-8B-Instruct
2025-04-26 15:44:50,001 - llama_model - INFO - 使用设备: cuda
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]
Device set to use cuda:0
2025-04-26 15:44:56,005 - llama_model - INFO - 模型加载完成！
2025-04-26 15:44:56,005 - parser_node - INFO - 创建ICL解析器，任务: qp
prompts/extract_qp.txt
2025-04-26 15:44:56,005 - parser_node - INFO - 运行qp_parser解析器
处理测试数据:   0%|          | 0/24 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:   4%|▍         | 1/24 [00:18<07:04, 18.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:   8%|▊         | 2/24 [00:39<07:18, 19.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  12%|█▎        | 3/24 [01:04<07:49, 22.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  17%|█▋        | 4/24 [01:21<06:40, 20.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  21%|██        | 5/24 [01:43<06:35, 20.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  25%|██▌       | 6/24 [02:01<05:57, 19.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  29%|██▉       | 7/24 [02:21<05:38, 19.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  33%|███▎      | 8/24 [02:35<04:50, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  38%|███▊      | 9/24 [02:57<04:47, 19.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  42%|████▏     | 10/24 [03:19<04:41, 20.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  46%|████▌     | 11/24 [03:36<04:10, 19.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  50%|█████     | 12/24 [03:57<03:57, 19.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  54%|█████▍    | 13/24 [04:14<03:27, 18.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  58%|█████▊    | 14/24 [04:34<03:12, 19.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  62%|██████▎   | 15/24 [04:58<03:05, 20.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  67%|██████▋   | 16/24 [05:19<02:45, 20.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  71%|███████   | 17/24 [05:49<02:46, 23.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  75%|███████▌  | 18/24 [06:10<02:16, 22.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  79%|███████▉  | 19/24 [06:28<01:46, 21.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  83%|████████▎ | 20/24 [06:52<01:28, 22.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  88%|████████▊ | 21/24 [07:11<01:03, 21.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  92%|█████████▏| 22/24 [07:35<00:43, 21.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据:  96%|█████████▌| 23/24 [08:03<00:23, 23.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
处理测试数据: 100%|██████████| 24/24 [08:17<00:00, 20.85s/it]处理测试数据: 100%|██████████| 24/24 [08:17<00:00, 20.72s/it]
已保存10个结果到results/Train_llama_0.6_0.3_0.1_results_qp_step2_20250426_154252.json
已保存20个结果到results/Train_llama_0.6_0.3_0.1_results_qp_step2_20250426_154252.json
处理完成，共24个结果已保存到results/Train_llama_0.6_0.3_0.1_results_qp_step2_20250426_154252.json
2025-04-26 15:53:13,193 - pipeline_builder - INFO - 运行节点: verifier
2025-04-26 15:53:13,193 - verifier_node - INFO - 加载Llama模型: /datacenter/models/LLM-Research/Llama-3-8B-Instruct
2025-04-26 15:53:13,193 - llama_model - INFO - 正在加载模型: /datacenter/models/LLM-Research/Llama-3-8B-Instruct
2025-04-26 15:53:13,539 - llama_model - INFO - 使用设备: cuda
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]
Device set to use cuda:0
2025-04-26 15:53:19,562 - llama_model - INFO - 模型加载完成！
2025-04-26 15:53:19,562 - verifier_node - INFO - 创建LLM验证器
2025-04-26 15:53:19,562 - verifier_node - INFO - 运行verifier验证器
处理测试数据:   0%|          | 0/24 [00:00<?, ?it/s]处理测试数据: 100%|██████████| 24/24 [00:00<00:00, 23735.75it/s]
已保存10个结果到results/Train_llama_0.6_0.3_0.1_results_final_20250426_154252.json
已保存20个结果到results/Train_llama_0.6_0.3_0.1_results_final_20250426_154252.json
处理完成，共24个结果已保存到results/Train_llama_0.6_0.3_0.1_results_final_20250426_154252.json
2025-04-26 15:53:19,564 - pipeline_runner - INFO - 保存结果: results/Train_llama_0.6_0.3_0.1_results.json
